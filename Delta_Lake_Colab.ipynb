{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XAOMzBehZ8-k",
        "6golaYZQg2iO",
        "kxyLh1n1aodW",
        "hbLwGeLVbTgn",
        "YtRhikqzb0re",
        "eGryv2cKd6CL",
        "pN2ayiqmf8Zz",
        "5XiQolC7gBqZ"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNLZiX0H0krmynQ8k9PyzPg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CynicDog/delta-lake-lab/blob/main/Delta_Lake_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to Delta Lake Colab!\n",
        "\n",
        "Delta Lake is an open-source storage layer that brings **ACID transactions, scalable metadata handling, and unifies streaming and batch data processing** on top of your existing data lake (like Parquet on S3 or local storage).  \n",
        "\n",
        "This notebook is designed for **hands-on experimentation** with Delta Lake using **PySpark**. You will be able to:\n",
        "\n",
        "- Create, read, and write Delta tables  \n",
        "- Explore Delta table features such as **updates, deletes, and merges**  \n",
        "- Examine Delta table history and the `_delta_log`  \n",
        "- Experiment with **time travel** to query older versions of data  \n",
        "\n",
        "Additionally, this is a space to explore **Databricks features and solutions** in a Colab environment, understand how they work, and implement them yourself."
      ],
      "metadata": {
        "id": "XAOMzBehZ8-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation"
      ],
      "metadata": {
        "id": "6golaYZQg2iO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall any existing conflicting packages to avoid version conflicts.\n",
        "# - pyspark: removes any pre-installed PySpark version\n",
        "# - delta-spark: removes any previous Delta Lake Python package\n",
        "# - dataproc-spark-connect: removes Google Colab’s built-in Spark connect package\n",
        "!pip uninstall -y pyspark delta-spark dataproc-spark-connect\n",
        "\n",
        "# Install compatible versions:\n",
        "# - PySpark 3.5.1: works with Delta Lake 3.2.0\n",
        "# - delta-spark 3.2.0: Delta Lake Python library\n",
        "!pip install -q pyspark==3.5.1 delta-spark==3.2.0\n",
        "\n",
        "def get_spark():\n",
        "    \"\"\"Creates and returns a SparkSession configured for Delta Lake.\n",
        "\n",
        "    This function sets up a SparkSession with the necessary Delta Lake\n",
        "    extensions and catalog, ensuring that Delta features such as\n",
        "    time travel, updates, and deletes are available.\n",
        "\n",
        "    Returns:\n",
        "        pyspark.sql.SparkSession: Configured SparkSession for Delta Lake.\n",
        "    \"\"\"\n",
        "    from pyspark.sql import SparkSession\n",
        "    from delta import configure_spark_with_delta_pip\n",
        "\n",
        "    # Build the SparkSession with Delta Lake configurations\n",
        "    builder = (\n",
        "        SparkSession.builder.appName(\"DeltaLakeApp\")\n",
        "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
        "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "    )\n",
        "\n",
        "    # Apply Delta Lake pip configuration and return the SparkSession\n",
        "    return configure_spark_with_delta_pip(builder).getOrCreate()\n",
        "\n",
        "spark = get_spark()\n",
        "spark"
      ],
      "metadata": {
        "id": "JZn6QObJg7ej",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "3b1b1e4f-ec3b-452f-a3e8-f7c0280428bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping pyspark as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping delta-spark as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping dataproc-spark-connect as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x78fcc9073020>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://dbaf8a967e38:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>DeltaLakeApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "kxyLh1n1aodW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Use Cases\n",
        "\n",
        "Delta Lake is widely used across organizations of all sizes because it provides a reliable and scalable foundation for managing large volumes of data in analytics and AI workflows. One of its most common applications is modernizing existing data lakes: by adding ACID transactions, schema enforcement, and scalable metadata handling on top of open storage formats, Delta Lake helps teams resolve the long-standing issues of unreliable or inconsistent data that often hinder traditional lakes. Many organizations also use Delta Lake as part of a lakehouse architecture to support <u>data warehousing techniques, enabling fast and dependable SQL analytics</u> while still maintaining the <u>flexibility and cost-efficiency of a data lake environment</u>. Because Delta Lake unifies batch and streaming data, it plays a central role in real-time data processing workloads, allowing developers to ingest continuous streams while applying the same transformation logic used for historical batch data.\n",
        "\n",
        "Beyond analytics, Delta Lake is a key component in machine learning and data science pipelines. It provides teams with a consistent and high-quality source of truth, ensuring that training datasets remain accurate, reproducible, and easy to version. Data engineering teams rely on Delta Lake to build robust pipelines that maintain data quality across ingestion, transformation, and operationalization stages. At the same time, business intelligence users benefit from its SQL accessibility, which makes it simple to query data directly from dashboards and reporting tools. Overall, Delta Lake’s emphasis on reliability, performance, and openness makes it an essential platform for data engineers, data scientists, and analysts working across modern big data ecosystems."
      ],
      "metadata": {
        "id": "nr6ixrrXbQvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Features\n",
        "\n",
        "Delta Lake introduces a number of core capabilities that form the backbone of the lakehouse paradigm. At its foundation, Delta Lake provides ACID transactions, ensuring that every data modification is executed safely and consistently, even under concurrent workloads or unexpected failures. These transactional guarantees are made possible by Delta Lake’s scalable metadata handling, which uses an append-only transaction log to record every change to a table. This design allows Delta Lake to manage very large datasets without suffering from the metadata bottlenecks common in raw data lakes.\n",
        "\n",
        "One of the most powerful capabilities enabled by the transaction log is time travel, which allows users to query earlier versions of a table by version number or timestamp. This feature is particularly valuable for debugging, validating model inputs, recovering from accidental deletions, or meeting audit and regulatory requirements. Delta Lake also unifies batch and streaming processing, allowing Spark Structured Streaming jobs to operate with the same APIs and logic used for batch workloads, while the underlying storage guarantees preserve correctness and consistency in both modes.\n",
        "\n",
        "To maintain data quality, Delta Lake enforces schemas on write and supports controlled schema evolution, preventing corrupted or malformed data from entering pipelines while still allowing tables to adapt as requirements change. Delta Lake also tracks a complete audit history of all operations, enabling transparency into who made changes and when. Modern workloads rely heavily on DML operations—such as updates, deletes, and merges—and Delta Lake provides efficient support for these across multiple execution engines and languages. Its open-source nature encourages broad adoption and collaboration, while its performance optimizations ensure that most workloads run efficiently without extensive tuning. Taken together, these features create a storage layer that is both powerful and approachable for a wide range of data professionals."
      ],
      "metadata": {
        "id": "yJ7K_Q47GTJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anatomy of a Delta Lake Table\n",
        "\n",
        "A Delta Lake table is composed of several tightly integrated components that together provide reliable storage, strong transactional guarantees, and efficient performance on large datasets. Each part of the table contributes to how Delta Lake manages data, tracks changes, and scales across distributed environments. Understanding these components makes it easier to reason about Delta Lake’s behavior, optimize pipelines, and work more effectively with the lakehouse format.\n",
        "\n",
        "### Data Files\n",
        "\n",
        "At the base of every Delta Lake table are the data files themselves, stored in the Parquet format. These files hold the raw records and are distributed across object stores or file systems such as HDFS, Amazon S3, Azure Data Lake Storage (ADLS), Google Cloud Storage, or MinIO. Parquet is chosen because its columnar layout, compression, and encoding techniques make it highly efficient for analytical queries and large-scale processing. Delta Lake does not alter the Parquet format but enhances its reliability by layering transactional control and metadata management on top of it.\n",
        "\n",
        "### Transaction Log\n",
        "\n",
        "Above the data files sits the transaction log—often referred to as the **_delta_log**—which is the heart of Delta Lake’s architecture. This log is a chronological sequence of JSON entries, each representing a single transaction against the table. Every change, whether inserting new data files, removing outdated ones, or modifying table metadata, is written as a new log entry. By recording operations rather than mutating files directly, the transaction log guarantees ACID semantics: all changes are atomic, consistent, isolated, and durable. This log is the mechanism that makes time travel, concurrent writes, schema enforcement, and recoverability possible.\n",
        "\n",
        "### Metadata\n",
        "\n",
        "The metadata tracked in the _delta_log describes the structure, layout, and configuration of the table. It includes information such as the table’s schema, partition columns, data skipping statistics, and protocol versions supported by the client. Metadata can be accessed programmatically through Spark, SQL, Python, or Rust APIs, giving users full insight into how the table is organized and how it has evolved. This metadata layer enables Delta Lake to optimize queries, enforce structural constraints, and adapt as data grows or workloads change.\n",
        "\n",
        "### Schema\n",
        "\n",
        "A Delta Lake table’s schema defines the structure of its data, including column names, data types, and nested fields. The schema is enforced whenever data is written, preventing corrupted or mismatched records from entering the table. Delta Lake also supports schema evolution, allowing new columns to be added or existing structures to change without breaking downstream processes. Because the schema and its modifications are captured in the transaction log, every version of the table retains a complete understanding of how the data was structured at that point in time.\n",
        "\n",
        "### Checkpoints\n",
        "\n",
        "To improve performance when reading table history, Delta Lake periodically writes **checkpoints**, which are compact Parquet summaries of the current state of the table. Instead of replaying every JSON log entry from the beginning, readers can load the most recent checkpoint and then apply only the newer transactions that follow it. By default, a checkpoint is generated every ten commits. This optimization significantly speeds up table initialization, reduces metadata overhead, and allows large tables to remain responsive even at massive scale.\n"
      ],
      "metadata": {
        "id": "lqk7laUHGbsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started"
      ],
      "metadata": {
        "id": "hbLwGeLVbTgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Basic CRUD Operations with Delta Tables"
      ],
      "metadata": {
        "id": "YtRhikqzb0re"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The table contains sample data with `id`, `name`, and `amount` columns and is saved to `/tmp/delta-crud-table`.  \n",
        "We use `overwrite` mode to replace any existing data. Delta Lake automatically maintains a transaction log (`_delta_log`) for ACID compliance and time travel."
      ],
      "metadata": {
        "id": "tW9kzYT5cd6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from delta.tables import DeltaTable\n",
        "\n",
        "# Create: Create a Delta table from a PySpark DataFrame\n",
        "data = [(1, \"Alice\", 100), (2, \"Bob\", 200), (3, \"Charlie\", 300)]\n",
        "columns = [\"id\", \"name\", \"amount\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "delta_path = \"/tmp/delta-crud-table\"\n",
        "\n",
        "# Save the DataFrame as a Delta table (overwrite mode)\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)"
      ],
      "metadata": {
        "id": "HkGgHRGjcP9j"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell loads the Delta table from the specified path into a PySpark DataFrame. We then display its contents using `show()`.  \n",
        "Delta Lake ensures we always read a consistent snapshot of the table, even after updates or deletes."
      ],
      "metadata": {
        "id": "2Bn5a-4ac0oC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read: Load the Delta table into a DataFrame\n",
        "delta_df = spark.read.format(\"delta\").load(delta_path)\n",
        "delta_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6pkiC0uck7F",
        "outputId": "28646ae7-595c-4cb8-e406-f256aa28bf26"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+\n",
            "| id|   name|amount|\n",
            "+---+-------+------+\n",
            "|  2|    Bob|   200|\n",
            "|  3|Charlie|   300|\n",
            "|  1|  Alice|   100|\n",
            "+---+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update Bob's `amount` to 250 using a `DeltaTable` object. The `condition` selects the rows, and `set` specifies the column to update. Delta Lake ensures the update is transactional and consistent."
      ],
      "metadata": {
        "id": "JIO1ORxTdIdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update: Update records in Delta table\n",
        "delta_table = DeltaTable.forPath(spark, delta_path)\n",
        "\n",
        "# Update Bob's amount to 250\n",
        "delta_table.update(\n",
        "    condition=\"name = 'Bob'\",  # Rows matching this condition will be updated\n",
        "    set={\"amount\": \"250\"}      # Columns to update\n",
        ")\n",
        "delta_table.toDF().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGi6lVDdcn12",
        "outputId": "2956cd8a-83b1-4950-8116-de9355e18197"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+\n",
            "| id|   name|amount|\n",
            "+---+-------+------+\n",
            "|  2|    Bob|   250|\n",
            "|  3|Charlie|   300|\n",
            "|  1|  Alice|   100|\n",
            "+---+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delete Alice's row using the `condition` parameter. Delta Lake ensures the deletion is transactional and preserves table consistency."
      ],
      "metadata": {
        "id": "u8OmXkUXdTuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete: Delete records from Delta table\n",
        "# Delete Alice's row\n",
        "delta_table.delete(condition=\"name = 'Alice'\")\n",
        "delta_table.toDF().show()"
      ],
      "metadata": {
        "id": "7FxWM1f_cply",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c347cbee-4fae-486e-efde-ef34124002c9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+\n",
            "| id|   name|amount|\n",
            "+---+-------+------+\n",
            "|  2|    Bob|   250|\n",
            "|  3|Charlie|   300|\n",
            "+---+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Merging / Upserting Data"
      ],
      "metadata": {
        "id": "eGryv2cKd6CL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delta Lake provides a powerful mechanism for upserting data into existing tables through the DeltaTable API and the DeltaMergeBuilder. A merge operation lets you define how incoming records should interact with existing ones by specifying a matching condition and separate actions for matched and unmatched rows. When the condition identifies a matching record, Delta Lake can update the existing row; when no match is found, a new row is inserted. Both operations are chained into a single merge() call, and because the entire merge is treated as one ACID transaction, the table always remains in a consistent state even under concurrent workloads or failures.\n",
        "\n",
        "With the DeltaTable API, you use a class called the `DeltaMergeBuilder` to define how new data should be merged into an existing table. Each combination of matching condition and action has its own method—`whenMatchedUpdate()` for updates and `whenNotMatchedInsert()` for inserts.  \n",
        "\n",
        "In this example, we merge a DataFrame of new records into the Delta table: rows with matching `id`s are updated, while new rows are inserted. Chaining these actions together in a single `merge()` ensures that each operation is atomic and that the table remains consistent."
      ],
      "metadata": {
        "id": "mSaESvwud-AY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from delta.tables import DeltaTable\n",
        "\n",
        "# New data to merge (upsert) into the Delta table\n",
        "data = [\n",
        "    (3, \"Charlie\", 350),  # Existing ID, will update\n",
        "    (4, \"David\", 400)     # New ID, will insert\n",
        "]\n",
        "new_df = spark.createDataFrame(data, [\"id\", \"name\", \"amount\"])\n",
        "\n",
        "# Perform merge (upsert) operation\n",
        "delta_table.alias(\"target\").merge(\n",
        "    source=new_df.alias(\"source\"),           # New data\n",
        "    condition=\"target.id = source.id\"        # Matching condition\n",
        ").whenMatchedUpdate(\n",
        "    set={\n",
        "        \"name\": \"source.name\",               # Update existing rows\n",
        "        \"amount\": \"source.amount\"\n",
        "    }\n",
        ").whenNotMatchedInsert(\n",
        "    values={\n",
        "        \"id\": \"source.id\",                   # Insert new rows\n",
        "        \"name\": \"source.name\",\n",
        "        \"amount\": \"source.amount\"\n",
        "    }\n",
        ").execute()\n",
        "\n",
        "# Show table after merge/upsert\n",
        "delta_table.toDF().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQh1pXgId-P6",
        "outputId": "9dc46688-74c8-4a74-876b-bb1f3e9ff575"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+------+\n",
            "| id|   name|amount|\n",
            "+---+-------+------+\n",
            "|  2|    Bob|   250|\n",
            "|  3|Charlie|   350|\n",
            "|  4|  David|   400|\n",
            "+---+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ACID in Depth"
      ],
      "metadata": {
        "id": "pN2ayiqmf8Zz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delta Lake provides **ACID (Atomicity, Consistency, Isolation, Durability) guarantees** on top of your data lake.  \n",
        "This ensures that every operation—whether a simple write, update, delete, or merge—is transactional and consistent, even in the presence of concurrent operations or failures.\n",
        "\n",
        "Delta Lake achieves this using the **_delta_log** folder, which tracks all changes made to the table as a sequence of JSON and checkpoint files."
      ],
      "metadata": {
        "id": "IPgMCQr_f-YK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The `_delta_log` Folder"
      ],
      "metadata": {
        "id": "5XiQolC7gBqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `_delta_log` folder contains:\n",
        "\n",
        "- **JSON commit files** (`00000000000000000000.json`, etc.): Each file represents a single transaction and records all actions (add, remove, update) in that commit.  \n",
        "- **Checkpoint Parquet files** (`*.checkpoint.parquet`): Periodically created for faster table recovery and reducing the need to read all JSON files.  \n",
        "\n",
        "These files together allow Delta Lake to:\n",
        "\n",
        "- Track table history for **time travel**\n",
        "- Ensure **atomicity**: either a transaction fully completes or has no effect\n",
        "- Maintain **consistency**: table state always conforms to schema and constraints\n",
        "- Provide **isolation**: concurrent operations see consistent snapshots\n",
        "- Guarantee **durability**: committed changes survive crashes or failures"
      ],
      "metadata": {
        "id": "ytH4-RrBgF2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from delta import configure_spark_with_delta_pip\n",
        "from delta.tables import DeltaTable\n",
        "import os\n",
        "spark = get_spark()\n",
        "\n",
        "# Create initial DataFrame\n",
        "data = [(1, \"Alice\", 100), (2, \"Bob\", 200)]\n",
        "columns = [\"id\", \"name\", \"amount\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Define Delta table path\n",
        "delta_path = \"/tmp/acid_demo_table\"\n",
        "\n",
        "# Write DataFrame as Delta table (initial commit)\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
        "\n",
        "# Perform additional operations to generate multiple commits\n",
        "delta_table = DeltaTable.forPath(spark, delta_path)"
      ],
      "metadata": {
        "id": "uVNmSI5YgFmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running this command lists all the files in the `_delta_log` folder of the Delta table. Each `.json` file corresponds to a single transaction, recording all the actions that were performed, such as adding, updating, or deleting data. The `_commits` file contains metadata about the committed transactions. The numbering of the JSON files reflects the sequence of operations, which Delta Lake uses to enforce ACID guarantees and enable time travel."
      ],
      "metadata": {
        "id": "r7xAOP75iKxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List all files in the Delta table log folder\n",
        "!ls /tmp/acid_demo_table/_delta_log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZVl7XVnh_bK",
        "outputId": "847747a1-07d6-48b4-f50c-abbce281f782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "00000000000000000000.json  00000000000000000003.json  00000000000000000006.json\n",
            "00000000000000000001.json  00000000000000000004.json  00000000000000000007.json\n",
            "00000000000000000002.json  00000000000000000005.json  _commits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2> Understanding a Delta Lake Commit JSON File </h2>\n",
        "\n",
        "When you write data to a Delta table, Delta Lake records the operation as a **commit** in the `_delta_log` folder. Each commit is stored as a **JSON file**, such as `00000000000000000000.json`. This file is crucial because it is **the atomic record of the transaction** and contains all the metadata, schema information, and file operations associated with that write.\n",
        "\n",
        "Let’s dissect the structure and fields of the JSON you posted:\n",
        "\n",
        "<h3> 1. <code>commitInfo</code> Object </h3>\n",
        "\n",
        "```json\n",
        "{\"commitInfo\":{\n",
        "    \"timestamp\":1763730403573,\n",
        "    \"operation\":\"WRITE\",\n",
        "    \"operationParameters\":{\"mode\":\"Overwrite\",\"partitionBy\":\"[]\"},\n",
        "    \"isolationLevel\":\"Serializable\",\n",
        "    \"isBlindAppend\":false,\n",
        "    \"operationMetrics\":{\"numFiles\":\"2\",\"numOutputRows\":\"2\",\"numOutputBytes\":\"1934\"},\n",
        "    \"engineInfo\":\"Apache-Spark/3.5.1 Delta-Lake/3.2.0\",\n",
        "    \"txnId\":\"1e8029ec-2cf7-4712-8c72-2b18bf4a35e0\"\n",
        "}}\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* **`timestamp`** – The exact time the commit occurred (in milliseconds since epoch). Useful for time travel queries.\n",
        "* **`operation`** – The type of operation performed (`WRITE`, `UPDATE`, `DELETE`, `MERGE`, etc.). In this case, it’s a `WRITE`.\n",
        "* **`operationParameters`** – Additional info about the operation:\n",
        "\n",
        "  * `mode`: Write mode (`Overwrite` here).\n",
        "  * `partitionBy`: Any partition columns used (empty array here).\n",
        "* **`isolationLevel`** – The transactional isolation level used (`Serializable` ensures full ACID isolation).\n",
        "* **`isBlindAppend`** – Indicates if the operation is a blind append (true if data is appended without checking existing files). Here it’s false.\n",
        "* **`operationMetrics`** – Metrics about the commit:\n",
        "\n",
        "  * `numFiles`: Number of files written (2).\n",
        "  * `numOutputRows`: Number of rows written (2).\n",
        "  * `numOutputBytes`: Approximate size in bytes (1934).\n",
        "* **`engineInfo`** – Spark and Delta versions used for the operation.\n",
        "* **`txnId`** – Unique identifier for this transaction. Every commit has a unique ID to track operations.\n",
        "\n",
        "**Key insight:** This object gives **a full audit trail of what the transaction did**, including metrics and configuration.\n",
        "\n",
        "<h3> 2. <code>metaData</code> Object </h3>\n",
        "\n",
        "```json\n",
        "{\"metaData\":{\n",
        "    \"id\":\"f4d6ad9d-c095-4c34-bb48-64b4011a43c3\",\n",
        "    \"format\":{\"provider\":\"parquet\",\"options\":{}},\n",
        "    \"schemaString\":\"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true},{\\\"name\\\":\\\"amount\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true}]}\",\n",
        "    \"partitionColumns\":[],\n",
        "    \"configuration\":{},\n",
        "    \"createdTime\":1763730402440\n",
        "}}\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* **`id`** – A unique identifier for the table metadata.\n",
        "* **`format`** – The storage format (`parquet`) and any specific options.\n",
        "* **`schemaString`** – The table schema serialized as a JSON string. Here we have three columns:\n",
        "\n",
        "  * `id` (long, nullable)\n",
        "  * `name` (string, nullable)\n",
        "  * `amount` (long, nullable)\n",
        "* **`partitionColumns`** – Lists any columns used for partitioning (empty here).\n",
        "* **`configuration`** – Any table-level configuration properties. Empty in this simple example.\n",
        "* **`createdTime`** – Timestamp when the table metadata was created.\n",
        "\n",
        "**Key insight:** This object records **the schema of the table** at this commit. Delta uses this to validate writes and enable schema evolution.\n",
        "\n",
        "<h3> 3. <code>protocol</code> Object </h3>\n",
        "\n",
        "```json\n",
        "{\"protocol\":{\"minReaderVersion\":1,\"minWriterVersion\":2}}\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* **`minReaderVersion`** – Minimum Delta protocol version required to read this table.\n",
        "* **`minWriterVersion`** – Minimum Delta protocol version required to write to this table.\n",
        "\n",
        "**Key insight:** The protocol object ensures **compatibility across different Delta Lake versions**, so older readers/writers know if they can interact with this table safely.\n",
        "\n",
        "<h3> 4. <code>add</code> Objects </h3>\n",
        "\n",
        "```json\n",
        "{\"add\":{\n",
        "    \"path\":\"part-00000-d40ffd41-6e37-447c-9ec7-e1ec58f31ef1-c000.snappy.parquet\",\n",
        "    \"partitionValues\":{},\n",
        "    \"size\":974,\n",
        "    \"modificationTime\":1763730402871,\n",
        "    \"dataChange\":true,\n",
        "    \"stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":1,\\\"name\\\":\\\"Alice\\\",\\\"amount\\\":100},\\\"maxValues\\\":{\\\"id\\\":1,\\\"name\\\":\\\"Alice\\\",\\\"amount\\\":100},\\\"nullCount\\\":{\\\"id\\\":0,\\\"name\\\":0,\\\"amount\\\":0}}\"\n",
        "}}\n",
        "```\n",
        "\n",
        "* **`path`** – The relative file path of the Parquet file that was added.\n",
        "* **`partitionValues`** – Partition values for the file (empty here since the table isn’t partitioned).\n",
        "* **`size`** – File size in bytes.\n",
        "* **`modificationTime`** – File timestamp.\n",
        "* **`dataChange`** – `true` indicates this commit modifies data. `false` would indicate metadata-only changes.\n",
        "* **`stats`** – Statistics about this file:\n",
        "\n",
        "  * `numRecords`: Number of rows in the file.\n",
        "  * `minValues` / `maxValues`: Min/max per column.\n",
        "  * `nullCount`: Count of nulls per column.\n",
        "\n",
        "**Key insight:** Each `add` entry describes **exactly what Parquet files were added** in this transaction. Delta uses this to maintain ACID guarantees and efficiently plan queries.\n",
        "\n",
        "<h3> Putting it all together </h3>\n",
        "\n",
        "1. **`commitInfo`** – Audit trail of the transaction.\n",
        "2. **`metaData`** – Table schema and configuration at this commit.\n",
        "3. **`protocol`** – Ensures version compatibility.\n",
        "4. **`add` / `remove` / other actions** – Physical file changes for this commit.\n",
        "\n",
        "Delta Lake builds the table state **by replaying all commits** in `_delta_log`, combining all `add` and `remove` actions, while ensuring **atomicity, consistency, isolation, and durability**.\n",
        "\n",
        "This single JSON file is therefore **both a record of the operation and the foundation of Delta Lake’s ACID guarantees**."
      ],
      "metadata": {
        "id": "nhzciwJ5iubb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /tmp/acid_demo_table/_delta_log/00000000000000000000.json | jq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu-8Bez1iOIt",
        "outputId": "9d5fc91c-84d9-4723-c19d-ea67a07bd9c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;39m{\n",
            "  \u001b[0m\u001b[34;1m\"commitInfo\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
            "    \u001b[0m\u001b[34;1m\"timestamp\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1763730403573\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"operation\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"WRITE\"\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"operationParameters\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
            "      \u001b[0m\u001b[34;1m\"mode\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Overwrite\"\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[34;1m\"partitionBy\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"[]\"\u001b[0m\u001b[1;39m\n",
            "    \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"isolationLevel\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Serializable\"\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"isBlindAppend\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39mfalse\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"operationMetrics\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
            "      \u001b[0m\u001b[34;1m\"numFiles\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"2\"\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[34;1m\"numOutputRows\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"2\"\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[34;1m\"numOutputBytes\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"1934\"\u001b[0m\u001b[1;39m\n",
            "    \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"engineInfo\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Apache-Spark/3.5.1 Delta-Lake/3.2.0\"\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"txnId\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"1e8029ec-2cf7-4712-8c72-2b18bf4a35e0\"\u001b[0m\u001b[1;39m\n",
            "  \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
            "\u001b[1;39m}\u001b[0m\n",
            "\u001b[1;39m{\n",
            "  \u001b[0m\u001b[34;1m\"metaData\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
            "    \u001b[0m\u001b[34;1m\"id\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"f4d6ad9d-c095-4c34-bb48-64b4011a43c3\"\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"format\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
            "      \u001b[0m\u001b[34;1m\"provider\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"parquet\"\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[34;1m\"options\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{}\u001b[0m\u001b[1;39m\n",
            "    \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"schemaString\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"amount\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\"\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"partitionColumns\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m[]\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"configuration\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{}\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"createdTime\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1763730402440\u001b[0m\u001b[1;39m\n",
            "  \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
            "\u001b[1;39m}\u001b[0m\n",
            "\u001b[1;39m{\n",
            "  \u001b[0m\u001b[34;1m\"protocol\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
            "    \u001b[0m\u001b[34;1m\"minReaderVersion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"minWriterVersion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m2\u001b[0m\u001b[1;39m\n",
            "  \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
            "\u001b[1;39m}\u001b[0m\n",
            "\u001b[1;39m{\n",
            "  \u001b[0m\u001b[34;1m\"add\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
            "    \u001b[0m\u001b[34;1m\"path\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"part-00000-d40ffd41-6e37-447c-9ec7-e1ec58f31ef1-c000.snappy.parquet\"\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"partitionValues\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{}\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"size\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m974\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"modificationTime\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1763730402871\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"dataChange\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39mtrue\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"stats\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":1,\\\"name\\\":\\\"Alice\\\",\\\"amount\\\":100},\\\"maxValues\\\":{\\\"id\\\":1,\\\"name\\\":\\\"Alice\\\",\\\"amount\\\":100},\\\"nullCount\\\":{\\\"id\\\":0,\\\"name\\\":0,\\\"amount\\\":0}}\"\u001b[0m\u001b[1;39m\n",
            "  \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
            "\u001b[1;39m}\u001b[0m\n",
            "\u001b[1;39m{\n",
            "  \u001b[0m\u001b[34;1m\"add\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
            "    \u001b[0m\u001b[34;1m\"path\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"part-00001-29608a09-d354-4105-8408-c8c5e48c332e-c000.snappy.parquet\"\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"partitionValues\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{}\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"size\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m960\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"modificationTime\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1763730403109\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"dataChange\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39mtrue\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"stats\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":2,\\\"name\\\":\\\"Bob\\\",\\\"amount\\\":200},\\\"maxValues\\\":{\\\"id\\\":2,\\\"name\\\":\\\"Bob\\\",\\\"amount\\\":200},\\\"nullCount\\\":{\\\"id\\\":0,\\\"name\\\":0,\\\"amount\\\":0}}\"\u001b[0m\u001b[1;39m\n",
            "  \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
            "\u001b[1;39m}\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below cell updates Bob's `amount` to 250, deletes Alice's row, and appends new rows for Charlie and David.  \n",
        "Each operation creates a new commit in `_delta_log`, ensuring the table remains consistent and transactional."
      ],
      "metadata": {
        "id": "_XrXsT9Oj84j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update Bob's amount\n",
        "delta_table.update(\n",
        "    condition=\"name = 'Bob'\",\n",
        "    set={\"amount\": \"250\"}\n",
        ")\n",
        "\n",
        "# Delete Alice's row\n",
        "delta_table.delete(condition=\"name = 'Alice'\")\n",
        "\n",
        "# Insert new rows (append)\n",
        "new_data = [(3, \"Charlie\", 300), (4, \"David\", 400)]\n",
        "new_df = spark.createDataFrame(new_data, [\"id\", \"name\", \"amount\"])\n",
        "new_df.write.format(\"delta\").mode(\"append\").save(delta_path)"
      ],
      "metadata": {
        "id": "v_Ic_kxWj1W4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we update a row in a Delta table, Delta creates a commit JSON in `_delta_log` to record the transaction. The `commitInfo` section logs the operation type, timestamp, read version, isolation level, and detailed metrics like the number of rows updated and files added or removed.  \n",
        "\n",
        "The `remove` entry marks the old Parquet file that contained the outdated data as deleted, while the `add` entry points to a new Parquet file with the updated row. Delta never modifies files in place; instead, it replaces them atomically. This design preserves ACID guarantees, allows time travel queries, and keeps a complete, auditable history of all changes to the table.  \n",
        "\n",
        "By inspecting these commit files, you can see exactly what changed in each operation and how Delta manages consistent snapshots of the table over time."
      ],
      "metadata": {
        "id": "d-dM2OlmkhAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /tmp/acid_demo_table/_delta_log/00000000000000000001.json | jq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASOR80S3jp9_",
        "outputId": "8967d60c-f9e7-4f40-aa2e-f872304a6120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;39m{\n",
            "  \u001b[0m\u001b[34;1m\"commitInfo\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
            "    \u001b[0m\u001b[34;1m\"timestamp\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1763730410838\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"operation\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"UPDATE\"\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"operationParameters\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
            "      \u001b[0m\u001b[34;1m\"predicate\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"[\\\"(name#7709 = Bob)\\\"]\"\u001b[0m\u001b[1;39m\n",
            "    \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"readVersion\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m0\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"isolationLevel\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Serializable\"\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"isBlindAppend\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39mfalse\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"operationMetrics\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
            "      \u001b[0m\u001b[34;1m\"numRemovedFiles\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"1\"\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[34;1m\"numRemovedBytes\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"960\"\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[34;1m\"numCopiedRows\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"0\"\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[34;1m\"numDeletionVectorsAdded\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"0\"\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[34;1m\"numDeletionVectorsRemoved\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"0\"\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[34;1m\"numAddedChangeFiles\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"0\"\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[34;1m\"executionTimeMs\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"6669\"\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[34;1m\"numDeletionVectorsUpdated\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"0\"\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[34;1m\"scanTimeMs\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"6355\"\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[34;1m\"numAddedFiles\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"1\"\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[34;1m\"numUpdatedRows\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"1\"\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[34;1m\"numAddedBytes\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"959\"\u001b[0m\u001b[1;39m,\n",
            "      \u001b[0m\u001b[34;1m\"rewriteTimeMs\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"314\"\u001b[0m\u001b[1;39m\n",
            "    \u001b[1;39m}\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"engineInfo\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"Apache-Spark/3.5.1 Delta-Lake/3.2.0\"\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"txnId\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"30fc5c8f-24b5-4aec-86df-a1f5aac88792\"\u001b[0m\u001b[1;39m\n",
            "  \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
            "\u001b[1;39m}\u001b[0m\n",
            "\u001b[1;39m{\n",
            "  \u001b[0m\u001b[34;1m\"add\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
            "    \u001b[0m\u001b[34;1m\"path\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"part-00000-61f77628-413b-4a18-9183-46f88dced94f-c000.snappy.parquet\"\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"partitionValues\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{}\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"size\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m959\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"modificationTime\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1763730410823\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"dataChange\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39mtrue\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"stats\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":2,\\\"name\\\":\\\"Bob\\\",\\\"amount\\\":250},\\\"maxValues\\\":{\\\"id\\\":2,\\\"name\\\":\\\"Bob\\\",\\\"amount\\\":250},\\\"nullCount\\\":{\\\"id\\\":0,\\\"name\\\":0,\\\"amount\\\":0}}\"\u001b[0m\u001b[1;39m\n",
            "  \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
            "\u001b[1;39m}\u001b[0m\n",
            "\u001b[1;39m{\n",
            "  \u001b[0m\u001b[34;1m\"remove\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{\n",
            "    \u001b[0m\u001b[34;1m\"path\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;32m\"part-00001-29608a09-d354-4105-8408-c8c5e48c332e-c000.snappy.parquet\"\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"deletionTimestamp\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m1763730410835\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"dataChange\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39mtrue\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"extendedFileMetadata\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39mtrue\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"partitionValues\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m{}\u001b[0m\u001b[1;39m,\n",
            "    \u001b[0m\u001b[34;1m\"size\"\u001b[0m\u001b[1;39m: \u001b[0m\u001b[0;39m960\u001b[0m\u001b[1;39m\n",
            "  \u001b[1;39m}\u001b[0m\u001b[1;39m\n",
            "\u001b[1;39m}\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time Travel"
      ],
      "metadata": {
        "id": "KPkU2rOQmmyt"
      }
    }
  ]
}